{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Workspace\n",
    "# Create Compute Target\n",
    "# Resgistering Data\n",
    "# Creating conda Environment\n",
    "\n",
    "# Create Experiment\n",
    "# Register model in workspace\n",
    "# Deploying the model\n",
    "# Prepare your test data\n",
    "# Real time scoring using RestAPI Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, ComputeTarget, Datastore, Dataset, Experiment\n",
    "from azureml.core.compute import AmlCompute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Details of your AML workspace\n",
    "subscription_id = \"0f644ff4-434a-4446-a414-16ddb2b713ee\"\n",
    "resource_group = \"rg-sbihackathon\"\n",
    "workspace = \"ml-demo-explore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "ml_client = MLClient(\n",
    "    DefaultAzureCredential(), subscription_id, resource_group, workspace\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace already exist!\n",
      "\n",
      "Compute Targets:\n",
      "\t buf-batch-cluster : AmlCompute\n",
      "Datastores:\n",
      "\t azureml_globaldatasets : AzureBlob\n",
      "\t azureml : AzureBlob\n",
      "\t workspaceartifactstore : AzureBlob\n",
      "\t workspaceworkingdirectory : AzureFile\n",
      "\t workspacefilestore : AzureFile\n",
      "\t workspaceblobstore : AzureBlob\n",
      "Datasets:\n",
      "\t banktermdeposit\n",
      "\t buf-v1\n",
      "Experiments:\n",
      "\t exp-day1-no-1 : ml-demo-explore\n",
      "\t exp-day2-no1 : ml-demo-explore\n",
      "\t exp-day2-no2 : ml-demo-explore\n",
      "\n",
      " Workspace.create(name='ml-demo-explore', subscription_id='0f644ff4-434a-4446-a414-16ddb2b713ee', resource_group='rg-sbihackathon')\n"
     ]
    }
   ],
   "source": [
    "# Creating Workspace\n",
    "from azureml.core.workspace import WorkspaceException\n",
    "# from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "\n",
    "try:\n",
    "    # ws = Workspace.from_config() #fetches details from 'config.json'\n",
    "    # ia = InteractiveLoginAuthentication(tenant_id='7c0c36f5-af83-4c24-8844-9962e0163719')\n",
    "    # Alternate way to configure workspace\n",
    "    ws = Workspace.get(subscription_id='0f644ff4-434a-4446-a414-16ddb2b713ee',\n",
    "                    resource_group='rg-sbihackathon',\n",
    "                    name='ml-demo-explore')\n",
    "    \n",
    "    print('Workspace already exist!')\n",
    "    # print('Workspace already exist!',ws, sep='\\n')\n",
    "    print('\\nCompute Targets:')\n",
    "    if ws.compute_targets != {} or len(ws.compute_targets) != 0:\n",
    "        for compute_name in ws.compute_targets:\n",
    "            compute = ws.compute_targets[compute_name]\n",
    "            # compute = ComputeTarget._get(ws, compute_name)\n",
    "            print(\"\\t\",compute.name, \":\", compute.type)\n",
    "    else:\n",
    "        print(\"\\tNone\")\n",
    "    print('Datastores:')\n",
    "    if ws.datastores != {} or len(ws.datastores) != 0:\n",
    "        for datastore_name in ws.datastores:\n",
    "            # datastore = ws.datastores[datastore_name]\n",
    "            datastore = Datastore.get(ws, datastore_name)\n",
    "            print(\"\\t\",datastore.name, ':', datastore.datastore_type)\n",
    "    else:\n",
    "        print(\"\\tNone\")   \n",
    "    print('Datasets:')\n",
    "    if len(list(ws.datasets.keys())) != 0:\n",
    "        for dataset_name in list(ws.datasets.keys()):\n",
    "            dataset = Dataset.get_by_name(ws, dataset_name)\n",
    "            print('\\t',dataset.name)\n",
    "    else:\n",
    "        print(\"\\tNone\")\n",
    "    print('Experiments:')\n",
    "    # print(ws.experiments)\n",
    "    if ws.experiments != {} or len(ws.experiments) != 0:\n",
    "        for experiment_name in ws.experiments:\n",
    "            # experiment = Experiment._get_base_info_dict() (ws, experiment_name)\n",
    "            experiment = ws.experiments[experiment_name]\n",
    "            print(\"\\t\",experiment.name, ':', experiment.workspace.name)\n",
    "    else:\n",
    "        print(\"\\tNone\")\n",
    "        \n",
    "    print(\"\\n\",ws)\n",
    "except WorkspaceException as workspaceException:\n",
    "    print(\"[ERROR]: \", workspaceException)\n",
    "    # Creates the workspace\n",
    "    # ws=Workspace.create(workspace_name, \n",
    "    #                 resource_group=resource_name,\n",
    "    #                 create_resource_group=True,\n",
    "    #                 subscription_id=subscriptionID,\n",
    "    #                 location=\"East US\")\n",
    "    # ws.write_config('.azureml')\n",
    "except Exception as e:\n",
    "    print(\"[ERROR]: \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Target Details\n",
    "aml_compute_target = \"buf-batch-cluster\" #ALL SMALL LETTER, NO underscore, 16ws long only.\n",
    "experiment_name= 'demo_expirement'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'buf-batch-cluster' Compute Target already exist.\n"
     ]
    }
   ],
   "source": [
    "# Create Compute Target\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "try:\n",
    "    # aml_compute = AmlCompute(ws, aml_compute_target)\n",
    "    aml_compute = ml_client.compute.get(aml_compute_target)\n",
    "    print(f\"'{aml_compute_target}' Compute Target already exist.\")\n",
    "    \n",
    "except ComputeTargetException as ComputeNotFound:\n",
    "    print(ComputeNotFound.message)\n",
    "    #Method-1\n",
    "    # print(\"Creating new ComputeTarget :\",aml_compute_target)\n",
    "    # provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_D2_V2\",\n",
    "    #                                                         min_nodes = 1, \n",
    "    #                                                         max_nodes = 4,\n",
    "    #                                                         idle_seconds_before_scaledown=3000)    \n",
    "    # aml_compute = ComputeTarget.create(ws, aml_compute_target, provisioning_config)\n",
    "    # aml_compute.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "    #Method-2\n",
    "    # compute = AmlCompute(name=cluster_name, size='STANDARD_DS3_V2',\n",
    "    #                      max_instances=4)\n",
    "    # cluster = ml_client.compute.begin_create_or_update(compute)\n",
    "    # print(\"Created\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register data\n",
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "web_path = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"\n",
    "data_name = \"creditcard_defaults\"\n",
    "credit_data = Data(\n",
    "    name=data_name,\n",
    "    path=web_path,\n",
    "    type=AssetTypes.URI_FILE,\n",
    "    description=\"Dataset for credit card defaults\",\n",
    "    tags={\"source_type\": \"web\", \"source\": \"UCI ML Repo\"},\n",
    "    version=\"1.0.0\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset with name creditcard_defaults was registered to workspace, the dataset version is 1.0.0\n",
      "creation_context:\n",
      "  created_at: '2023-03-23T11:47:09.716646+00:00'\n",
      "  created_by: Mohammad Affan\n",
      "  created_by_type: User\n",
      "  last_modified_at: '2023-03-23T11:47:09.726126+00:00'\n",
      "description: Dataset for credit card defaults\n",
      "id: /subscriptions/0f644ff4-434a-4446-a414-16ddb2b713ee/resourceGroups/rg-sbihackathon/providers/Microsoft.MachineLearningServices/workspaces/ml-demo-explore/data/creditcard_defaults/versions/1.0.0\n",
      "name: creditcard_defaults\n",
      "path: https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\n",
      "properties: {}\n",
      "tags:\n",
      "  source: UCI ML Repo\n",
      "  source_type: web\n",
      "type: uri_file\n",
      "version: 1.0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Registering the data to your workspace so it becomes reusable across pipelines.\n",
    "credit_data = ml_client.data.create_or_update(credit_data)\n",
    "print(f\"Dataset with name {credit_data.name} was registered to workspace, the dataset version is {credit_data.version}\")\n",
    "\n",
    "# In the future, you can fetch the same dataset from the workspace using credit_dataset = ml_client.data.get(\"<DATA ASSET NAME>\", version='<VERSION>').\n",
    "# Azure Machine Learning mounts datasets as folders to the computes, therefore, we created an auxiliary select_first_file function to access the data file inside the mounted input folder.\n",
    "credit_dataset = ml_client.data.get(data_name, version=credit_data.version)\n",
    "print(credit_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a job Environment for pipeline\n",
    "'''\n",
    "So far, you've created a development environment on the compute instance, your development machine. \n",
    "    You'll also need an environment to use for each step of the pipeline. \n",
    "    Each step can have its own environment, or you can use some common environments for multiple steps.\n",
    "\n",
    "In this example, you'll create a conda environment for your jobs, using a conda yaml file. \n",
    "    First, create a directory to store the file in.\n",
    "'''\n",
    "import os\n",
    "\n",
    "dependencies_dir = \"./dependencies\"\n",
    "os.makedirs(dependencies_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create and write the file (conda.yml) in the dependencies directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./dependencies/conda.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {dependencies_dir}/conda.yml\n",
    "name: model-env\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.8\n",
    "  - numpy=1.21.2\n",
    "  - pip=21.2.4\n",
    "  - scikit-learn=0.24.2\n",
    "  - scipy=1.7.1\n",
    "  - pandas>=1.1,<1.2\n",
    "  - pip:\n",
    "    - inference-schema[numpy-support]==1.3.0\n",
    "    - xlrd==2.0.1\n",
    "    - mlflow== 1.26.1\n",
    "    - azureml-mlflow==1.42.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment with name aml-scikit-learn is registered to workspace, the environment version is 0.1.0\n"
     ]
    }
   ],
   "source": [
    "# Use the yaml file to create and register this custom environment in your workspace:\n",
    "from azure.ai.ml.entities import Environment\n",
    "\n",
    "custom_env_name = \"aml-scikit-learn\"\n",
    "\n",
    "pipeline_job_env = Environment(\n",
    "    name=custom_env_name,\n",
    "    description=\"Custom environment for Credit Card Defaults pipeline\",\n",
    "    tags={\"scikit-learn\": \"0.24.2\"},\n",
    "    conda_file=os.path.join(dependencies_dir, \"conda.yml\"),\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
    "    version=\"0.1.0\",)\n",
    "pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
    "print(f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the training pipeline, before that we'll create components and then from this component we'll create the pipeline\n",
    "\n",
    "# Create component 1: data prep (using programmatic definition)\n",
    "import os\n",
    "\n",
    "data_prep_src_dir = \"./components/data_prep\"\n",
    "os.makedirs(data_prep_src_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script performs the simple task of splitting the data into train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./components/data_prep/data_prep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {data_prep_src_dir}/data_prep.py\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "import mlflow\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function of the script.\"\"\"\n",
    "\n",
    "    # input and output arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--data\", type=str, help=\"path to input data\")\n",
    "    parser.add_argument(\"--test_train_ratio\", type=float, required=False, default=0.25)\n",
    "    parser.add_argument(\"--train_data\", type=str, help=\"path to train data\")\n",
    "    parser.add_argument(\"--test_data\", type=str, help=\"path to test data\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Start Logging\n",
    "    mlflow.start_run()\n",
    "\n",
    "    print(\" \".join(f\"{k}={v}\" for k, v in vars(args).items()))\n",
    "\n",
    "    print(\"input data:\", args.data)\n",
    "\n",
    "    credit_df = pd.read_excel(args.data, header=1, index_col=0)\n",
    "\n",
    "    mlflow.log_metric(\"num_samples\", credit_df.shape[0])\n",
    "    mlflow.log_metric(\"num_features\", credit_df.shape[1] - 1)\n",
    "\n",
    "    credit_train_df, credit_test_df = train_test_split(\n",
    "        credit_df,\n",
    "        test_size=args.test_train_ratio,\n",
    "    )\n",
    "\n",
    "    # output paths are mounted as folder, therefore, we are adding a filename to the path\n",
    "    credit_train_df.to_csv(os.path.join(args.train_data, \"data.csv\"), index=False)\n",
    "\n",
    "    credit_test_df.to_csv(os.path.join(args.test_data, \"data.csv\"), index=False)\n",
    "\n",
    "    # Stop Logging\n",
    "    mlflow.end_run()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the general purpose CommandComponent that can run command line actions. \n",
    "# This command line action can directly call system commands or run a script. \n",
    "# The inputs/outputs are specified on the command line via the ${{ ... }} notation.\n",
    "\n",
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input, Output\n",
    "\n",
    "data_prep_component = command(\n",
    "    name=\"data_prep_credit_defaults\",\n",
    "    display_name=\"Data preparation for training\",\n",
    "    description=\"reads a .xl input, split the input to train and test\",\n",
    "    inputs={\n",
    "        \"data\": Input(type=\"uri_folder\"),\n",
    "        \"test_train_ratio\": Input(type=\"number\"),\n",
    "    },\n",
    "    outputs=dict(\n",
    "        train_data=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "        test_data=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "    ),\n",
    "    # The source folder of the component\n",
    "    code=data_prep_src_dir,\n",
    "    command=\"\"\"python data_prep.py \\\n",
    "            --data ${{inputs.data}} --test_train_ratio ${{inputs.test_train_ratio}} \\\n",
    "            --train_data ${{outputs.train_data}} --test_data ${{outputs.test_data}} \\\n",
    "            \"\"\",\n",
    "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create component 2: training (using yaml definition)\n",
    "# We'll create will consume the training and test data, train a tree based model and return the output model. You'll use Azure Machine Learning logging capabilities to record and visualize the learning progress.\n",
    "import os\n",
    "\n",
    "train_src_dir = \"./components/train\"\n",
    "os.makedirs(train_src_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training script in the directory:\n",
    "# In this training script, once the model is trained, \n",
    "# the model file is saved and registered to the workspace. \n",
    "# Now you can use the registered model in inferencing endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./components/train/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {train_src_dir}/train.py\n",
    "import argparse\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "\n",
    "\n",
    "def select_first_file(path):\n",
    "    \"\"\"Selects first file in folder, use under assumption there is only one file in folder\n",
    "    Args:\n",
    "        path (str): path to directory or file to choose\n",
    "    Returns:\n",
    "        str: full path of selected file\n",
    "    \"\"\"\n",
    "    files = os.listdir(path)\n",
    "    return os.path.join(path, files[0])\n",
    "\n",
    "\n",
    "# Start Logging\n",
    "mlflow.start_run()\n",
    "\n",
    "# enable autologging\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "os.makedirs(\"./outputs\", exist_ok=True)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function of the script.\"\"\"\n",
    "\n",
    "    # input and output arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--train_data\", type=str, help=\"path to train data\")\n",
    "    parser.add_argument(\"--test_data\", type=str, help=\"path to test data\")\n",
    "    parser.add_argument(\"--n_estimators\", required=False, default=100, type=int)\n",
    "    parser.add_argument(\"--learning_rate\", required=False, default=0.1, type=float)\n",
    "    parser.add_argument(\"--registered_model_name\", type=str, help=\"model name\")\n",
    "    parser.add_argument(\"--model\", type=str, help=\"path to model file\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # paths are mounted as folder, therefore, we are selecting the file from folder\n",
    "    train_df = pd.read_csv(select_first_file(args.train_data))\n",
    "\n",
    "    # Extracting the label column\n",
    "    y_train = train_df.pop(\"default payment next month\")\n",
    "\n",
    "    # convert the dataframe values to array\n",
    "    X_train = train_df.values\n",
    "\n",
    "    # paths are mounted as folder, therefore, we are selecting the file from folder\n",
    "    test_df = pd.read_csv(select_first_file(args.test_data))\n",
    "\n",
    "    # Extracting the label column\n",
    "    y_test = test_df.pop(\"default payment next month\")\n",
    "\n",
    "    # convert the dataframe values to array\n",
    "    X_test = test_df.values\n",
    "\n",
    "    print(f\"Training with data of shape {X_train.shape}\")\n",
    "\n",
    "    clf = GradientBoostingClassifier(\n",
    "        n_estimators=args.n_estimators, learning_rate=args.learning_rate\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Registering the model to the workspace\n",
    "    print(\"Registering the model via MLFlow\")\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=clf,\n",
    "        registered_model_name=args.registered_model_name,\n",
    "        artifact_path=args.registered_model_name,\n",
    "    )\n",
    "\n",
    "    # Saving the model to a file\n",
    "    mlflow.sklearn.save_model(\n",
    "        sk_model=clf,\n",
    "        path=os.path.join(args.model, \"trained_model\"),\n",
    "    )\n",
    "\n",
    "    # Stop Logging\n",
    "    mlflow.end_run()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the environment of this step, we'll use one of the built-in (curated) Azure ML environments. \n",
    "# The tag azureml, tells the system to use look for the name in curated environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./components/train/train.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {train_src_dir}/train.yml\n",
    "# <component>\n",
    "name: train_credit_defaults_model\n",
    "display_name: Train Credit Defaults Model\n",
    "# version: 1 # Not specifying a version will automatically update the version\n",
    "type: command\n",
    "inputs:\n",
    "  train_data: \n",
    "    type: uri_folder\n",
    "  test_data: \n",
    "    type: uri_folder\n",
    "  learning_rate:\n",
    "    type: number     \n",
    "  registered_model_name:\n",
    "    type: string\n",
    "outputs:\n",
    "  model:\n",
    "    type: uri_folder\n",
    "code: .\n",
    "environment:\n",
    "  # for this step, we'll use an AzureML curate environment\n",
    "  azureml:AzureML-sklearn-1.0-ubuntu20.04-py38-cpu:1\n",
    "command: >-\n",
    "  python train.py \n",
    "  --train_data ${{inputs.train_data}} \n",
    "  --test_data ${{inputs.test_data}} \n",
    "  --learning_rate ${{inputs.learning_rate}}\n",
    "  --registered_model_name ${{inputs.registered_model_name}} \n",
    "  --model ${{outputs.model}}\n",
    "# </component>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading train (0.0 MBs): 100%|##########| 3527/3527 [00:00<00:00, 19543.03it/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component train_credit_defaults_model with Version 1 is registered\n"
     ]
    }
   ],
   "source": [
    "# Now create and register the component:\n",
    "# importing the Component Package\n",
    "from azure.ai.ml import load_component\n",
    "\n",
    "# Loading the component from the yml file\n",
    "train_component = load_component(source=os.path.join(train_src_dir, \"train.yml\"))\n",
    "\n",
    "# Now we register the component to the workspace\n",
    "train_component = ml_client.create_or_update(train_component)\n",
    "# Create (register) the component in your workspace\n",
    "print(f\"Component {train_component.name} with Version {train_component.version} is registered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pipeline from components\n",
    "# Now that both your components are defined and registered, you can start implementing the pipeline.\n",
    "# the dsl decorator tells the sdk that we are defining an Azure ML pipeline\n",
    "from azure.ai.ml import dsl, Input, Output\n",
    "\n",
    "@dsl.pipeline(compute=aml_compute_target,\n",
    "            description=\"E2E data_perp-train pipeline\",)\n",
    "def credit_defaults_pipeline(pipeline_job_data_input, pipeline_job_test_train_ratio,\n",
    "                        pipeline_job_learning_rate, pipeline_job_registered_model_name,):\n",
    "    # using data_prep_function like a python call with its own inputs\n",
    "    data_prep_job = data_prep_component(data=pipeline_job_data_input,\n",
    "        test_train_ratio=pipeline_job_test_train_ratio,)\n",
    "\n",
    "    # using train_func like a python call with its own inputs\n",
    "    train_job = train_component(train_data=data_prep_job.outputs.train_data,  # note: using outputs from previous step\n",
    "        test_data=data_prep_job.outputs.test_data,  # note: using outputs from previous step\n",
    "        learning_rate=pipeline_job_learning_rate,  # note: using a pipeline input as parameter\n",
    "        registered_model_name=pipeline_job_registered_model_name,)\n",
    "    \n",
    "    # a pipeline returns a dictionary of outputs\n",
    "    # keys will code for the pipeline output identifier\n",
    "    return {\n",
    "        \"pipeline_job_train_data\": data_prep_job.outputs.train_data,\n",
    "        \"pipeline_job_test_data\": data_prep_job.outputs.test_data,\n",
    "    }\n",
    "# Using your pipeline definition to instantiate a pipeline with your dataset, \n",
    "# split rate of choice and the name you picked for your model.\n",
    "registered_model_name = \"credit_defaults_model\"\n",
    "\n",
    "# Let's instantiate the pipeline with the parameters of our choice\n",
    "pipeline = credit_defaults_pipeline(\n",
    "    pipeline_job_data_input=Input(type=\"uri_file\", path=credit_data.path),\n",
    "    pipeline_job_test_train_ratio=0.25,\n",
    "    pipeline_job_learning_rate=0.05,\n",
    "    pipeline_job_registered_model_name=registered_model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading data_prep (0.0 MBs): 100%|##########| 1433/1433 [00:00<00:00, 33208.31it/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Submit the job\n",
    "#  submit the job to run in Azure Machine Learning. This time you'll use create_or_update on ml_client.jobs\n",
    "# We'll also pass an experiment name. \n",
    "# An experiment is a container for all the iterations one does on a certain project. All the jobs submitted under the same experiment name would be listed next to each other in Azure Machine Learning studio.\n",
    "\n",
    "# Once completed, the pipeline will register a model in your workspace as a result of training.\n",
    "\n",
    "import webbrowser\n",
    "\n",
    "# submit the pipeline job\n",
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline,\n",
    "    # Project's name\n",
    "    experiment_name=\"e2e_registered_components\",\n",
    ")\n",
    "# open the pipeline in web browser\n",
    "webbrowser.open(pipeline_job.studio_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEPLOYMENT\n",
    "# Deploy the model as an online endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_dataset(dataset_id):\n",
    "    '''\n",
    "    Loads the previously used dataset.\n",
    "    It assumes that the script is run in an AzureML command job under the same workspace as the original experiment.\n",
    "    '''\n",
    "    from azureml.core.dataset import Dataset\n",
    "    from azureml.core.run import Run\n",
    "    ws = Run.get_context().experiment.workspace\n",
    "    dataset = Dataset.get_by_id(workspace=ws, id=dataset_id)\n",
    "    return dataset.to_pandas_dataframe()\n",
    "\n",
    "def split_dataset(X, y, weights, split_ratio, should_stratify):\n",
    "    '''\n",
    "    Splits the dataset into a training and testing set.\n",
    "    Splits the dataset using the given split ratio. The default ratio given is 0.25 but can be\n",
    "    changed in the main function. If should_stratify is true the data will be split in a stratified\n",
    "    way, meaning that each new set will have the same distribution of the target value as the\n",
    "    original dataset. should_stratify is true for a classification run, false otherwise.\n",
    "    '''\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    random_state = 42\n",
    "    if should_stratify:\n",
    "        stratify = y\n",
    "    else:\n",
    "        stratify = None\n",
    "\n",
    "    if weights is not None:\n",
    "        X_train, X_test, y_train, y_test, weights_train, weights_test = train_test_split(\n",
    "            X, y, weights, stratify=stratify, test_size=split_ratio, random_state=random_state)\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, stratify=stratify, test_size=split_ratio, random_state=random_state)\n",
    "        weights_train, weights_test = None, None\n",
    "    return (X_train, y_train, weights_train), (X_test, y_test, weights_test)\n",
    "\n",
    "def prepare_data(dataframe):\n",
    "    '''\n",
    "    Prepares data for training.\n",
    "    Cleans the data, splits out the feature and sample weight columns and prepares the data for use in training.\n",
    "    This function can vary depending on the type of dataset and the experiment task type: classification,\n",
    "    regression, or time-series forecasting.\n",
    "    '''\n",
    "    from azureml.training.tabular.preprocessing import data_cleaning\n",
    "    label_column_name = 'CustomerScore'\n",
    "    \n",
    "    # extract the features, target and sample weight arrays\n",
    "    y = dataframe[label_column_name].values\n",
    "    X = dataframe.drop([label_column_name], axis=1)\n",
    "    sample_weights = None\n",
    "    # Split training data into train/test datasets and take only the train dataset\n",
    "    split_ratio = 0.15\n",
    "    (X, y, sample_weights), _ = split_dataset(X, y, sample_weights, split_ratio, should_stratify=False)\n",
    "    X, y, sample_weights = data_cleaning._remove_nan_rows_in_X_y(X, y, sample_weights,\n",
    "        is_timeseries=False, target_column=label_column_name)\n",
    "    \n",
    "    return X, y, sample_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_transformation_config():\n",
    "    '''\n",
    "    Specifies the featurization step in the final scikit-learn pipeline.\n",
    "    If you have many columns that need to have the same featurization/transformation applied (for example,\n",
    "    50 columns in several column groups), these columns are handled by grouping based on type. Each column\n",
    "    group then has a unique mapper applied to all columns in the group.\n",
    "    '''\n",
    "    from sklearn.pipeline import FeatureUnion\n",
    "    column_group_0 = [['Average_Daily_Balance'], ['Current_Balance_of_Customer'], ['Credit_Limit'], ['Income_of_the_Customer']]\n",
    "    column_group_3 = ['Billing_State']\n",
    "    column_group_4 = ['AGE', 'Phone', 'YearOfBirth_of_Customer', 'Billing_Zip']\n",
    "    column_group_2 = ['CurrentBankBranchId', 'CustomerMonthsOnBooks', 'Email_Address', 'MonthOfBirth_of_Customer', 'Billing_Address_Line_1', 'Billing_City']\n",
    "    column_group_1 = ['Due_Date_Payment', 'Credit_Limit_Date']\n",
    "    column_group_5 = ['Phone', 'YearOfBirth_of_Customer', 'Billing_Zip']\n",
    "    \n",
    "    feature_union = FeatureUnion([\n",
    "        ('mapper_0', get_mapper_0(column_group_0)),\n",
    "        ('mapper_1', get_mapper_1(column_group_1)),\n",
    "        ('mapper_2', get_mapper_2(column_group_2)),\n",
    "        ('mapper_3', get_mapper_3(column_group_3)),\n",
    "        ('mapper_4', get_mapper_4(column_group_4)),\n",
    "        ('mapper_5', get_mapper_5(column_group_5)),])\n",
    "    return feature_union\n",
    "    \n",
    "def generate_preprocessor_config():\n",
    "    '''\n",
    "    Specifies a preprocessing step to be done after featurization in the final scikit-learn pipeline.\n",
    "    Normally, this preprocessing step only consists of data standardization/normalization that is\n",
    "    accomplished with sklearn.preprocessing. Automated ML only specifies a preprocessing step for\n",
    "    non-ensemble classification and regression models.\n",
    "    '''\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    preproc = StandardScaler(\n",
    "        copy=True,\n",
    "        with_mean=False,\n",
    "        with_std=False\n",
    "    )\n",
    "    return preproc\n",
    "\n",
    "def generate_algorithm_config():\n",
    "    '''\n",
    "    Specifies the actual algorithm and hyperparameters for training the model.\n",
    "    It is the last stage of the final scikit-learn pipeline. For ensemble models, generate_preprocessor_config_N()\n",
    "    (if needed) and generate_algorithm_config_N() are defined for each learner in the ensemble model,\n",
    "    where N represents the placement of each learner in the ensemble model's list. For stack ensemble\n",
    "    models, the meta learner generate_algorithm_config_meta() is defined.\n",
    "    '''\n",
    "    from sklearn.linear_model import ElasticNet\n",
    "    algorithm = ElasticNet(\n",
    "        alpha=0.36905263157894735,\n",
    "        copy_X=True,\n",
    "        fit_intercept=True,\n",
    "        l1_ratio=0.01,\n",
    "        max_iter=1000,\n",
    "        normalize=False,\n",
    "        positive=False,\n",
    "        precompute=False,\n",
    "        random_state=None,\n",
    "        selection='cyclic',\n",
    "        tol=0.0001,\n",
    "        warm_start=False\n",
    "    )    \n",
    "    return algorithm\n",
    "\n",
    "def build_model_pipeline():\n",
    "    '''\n",
    "    Defines the scikit-learn pipeline steps.\n",
    "    '''\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    pipeline = Pipeline(\n",
    "        steps=[\n",
    "            ('featurization', generate_data_transformation_config()),\n",
    "            ('preproc', generate_preprocessor_config()),\n",
    "            ('model', generate_algorithm_config()),\n",
    "        ]\n",
    "    )\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, y, sample_weights=None, transformer=None):\n",
    "    '''\n",
    "    Calls the fit() method to train the model.\n",
    "    The return value is the model fitted/trained on the input data.\n",
    "    '''\n",
    "    model_pipeline = build_model_pipeline()\n",
    "    model = model_pipeline.fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_training_dataset(training_dataset_id)\n",
    "X, y, sample_weights = prepare_data(df)\n",
    "split_ratio = 0.25\n",
    "(X_train, y_train, sample_weights_train), (X_valid, y_valid, sample_weights_valid) = split_dataset(X, y, sample_weights, split_ratio, should_stratify=False)\n",
    "model = train_model(X_train, y_train, sample_weights_train)\n",
    "print(model)\n",
    "# metrics = calculate_metrics(model, X, y, sample_weights, X_test=X_valid, y_test=y_valid)\n",
    "# metrics_log_methods = get_metrics_log_methods()\n",
    "# print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azure-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
